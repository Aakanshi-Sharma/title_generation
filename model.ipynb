{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff39fbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import Dense , LSTM, Dropout, Embedding\n",
    "from keras. models import Sequential\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d9993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DELL\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words=stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f2a2f02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bef2ae6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "      <th>terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>Stereo matching is one of the widely used tech...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>The recent advancements in artificial intellig...</td>\n",
       "      <td>['cs.CV', 'cs.AI', 'cs.LG']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>In this paper, we proposed a novel mutual cons...</td>\n",
       "      <td>['cs.CV', 'cs.AI']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>Consistency training has proven to be an advan...</td>\n",
       "      <td>['cs.CV']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>To ensure safety in automated driving, the cor...</td>\n",
       "      <td>['cs.CV', 'cs.LG']</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \\\n",
       "0  Stereo matching is one of the widely used tech...   \n",
       "1  The recent advancements in artificial intellig...   \n",
       "2  In this paper, we proposed a novel mutual cons...   \n",
       "3  Consistency training has proven to be an advan...   \n",
       "4  To ensure safety in automated driving, the cor...   \n",
       "\n",
       "                         terms  \n",
       "0           ['cs.CV', 'cs.LG']  \n",
       "1  ['cs.CV', 'cs.AI', 'cs.LG']  \n",
       "2           ['cs.CV', 'cs.AI']  \n",
       "3                    ['cs.CV']  \n",
       "4           ['cs.CV', 'cs.LG']  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "80e07676",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=[\"terms\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b922b10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Stereo matching is one of the widely used techniques for inferring depth from\\nstereo images owing to its robustness and speed. It has become one of the major\\ntopics of research since it finds its applications in autonomous driving,\\nrobotic navigation, 3D reconstruction, and many other fields. Finding pixel\\ncorrespondences in non-textured, occluded and reflective areas is the major\\nchallenge in stereo matching. Recent developments have shown that semantic cues\\nfrom image segmentation can be used to improve the results of stereo matching.\\nMany deep neural network architectures have been proposed to leverage the\\nadvantages of semantic segmentation in stereo matching. This paper aims to give\\na comparison among the state of art networks both in terms of accuracy and in\\nterms of speed which are of higher importance in real-time applications.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.summaries[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "de36e8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing_text(text):\n",
    "    lists=[]\n",
    "    text=text.lower()\n",
    "    text=text.replace('\\n', '')\n",
    "    text=text.strip().split(\" \")\n",
    "    for i in text:\n",
    "        if i not in stop_words:\n",
    "            lists.append(i)\n",
    "    returned_lists=[]\n",
    "    for i in lists:\n",
    "        arr=[]\n",
    "        for j in i:\n",
    "            if j not in string.punctuation:\n",
    "                arr.append(j)\n",
    "        \n",
    "        returned_lists.append(''.join(arr))\n",
    "    return returned_lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e6588af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stereo',\n",
       " 'matching',\n",
       " 'one',\n",
       " 'widely',\n",
       " 'used',\n",
       " 'techniques',\n",
       " 'inferring',\n",
       " 'depth',\n",
       " 'fromstereo',\n",
       " 'images',\n",
       " 'owing',\n",
       " 'robustness',\n",
       " 'speed',\n",
       " 'become',\n",
       " 'one',\n",
       " 'majortopics',\n",
       " 'research',\n",
       " 'since',\n",
       " 'finds',\n",
       " 'applications',\n",
       " 'autonomous',\n",
       " 'drivingrobotic',\n",
       " 'navigation',\n",
       " '3d',\n",
       " 'reconstruction',\n",
       " 'many',\n",
       " 'fields',\n",
       " 'finding',\n",
       " 'pixelcorrespondences',\n",
       " 'nontextured',\n",
       " 'occluded',\n",
       " 'reflective',\n",
       " 'areas',\n",
       " 'majorchallenge',\n",
       " 'stereo',\n",
       " 'matching',\n",
       " 'recent',\n",
       " 'developments',\n",
       " 'shown',\n",
       " 'semantic',\n",
       " 'cuesfrom',\n",
       " 'image',\n",
       " 'segmentation',\n",
       " 'used',\n",
       " 'improve',\n",
       " 'results',\n",
       " 'stereo',\n",
       " 'matchingmany',\n",
       " 'deep',\n",
       " 'neural',\n",
       " 'network',\n",
       " 'architectures',\n",
       " 'proposed',\n",
       " 'leverage',\n",
       " 'theadvantages',\n",
       " 'semantic',\n",
       " 'segmentation',\n",
       " 'stereo',\n",
       " 'matching',\n",
       " 'paper',\n",
       " 'aims',\n",
       " 'givea',\n",
       " 'comparison',\n",
       " 'among',\n",
       " 'state',\n",
       " 'art',\n",
       " 'networks',\n",
       " 'terms',\n",
       " 'accuracy',\n",
       " 'interms',\n",
       " 'speed',\n",
       " 'higher',\n",
       " 'importance',\n",
       " 'realtime',\n",
       " 'applications']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing_text('Stereo matching is one of the widely used techniques for inferring depth from\\nstereo images owing to its robustness and speed. It has become one of the major\\ntopics of research since it finds its applications in autonomous driving,\\nrobotic navigation, 3D reconstruction, and many other fields. Finding pixel\\ncorrespondences in non-textured, occluded and reflective areas is the major\\nchallenge in stereo matching. Recent developments have shown that semantic cues\\nfrom image segmentation can be used to improve the results of stereo matching.\\nMany deep neural network architectures have been proposed to leverage the\\nadvantages of semantic segmentation in stereo matching. This paper aims to give\\na comparison among the state of art networks both in terms of accuracy and in\\nterms of speed which are of higher importance in real-time applications.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162feecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['summaries']=df['summaries'].apply(preprocessing_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "834e7ba9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>titles</th>\n",
       "      <th>summaries</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Survey on Semantic Stereo Matching / Semantic ...</td>\n",
       "      <td>[stereo, matching, one, widely, used, techniqu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>FUTURE-AI: Guiding Principles and Consensus Re...</td>\n",
       "      <td>[recent, advancements, artificial, intelligenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enforcing Mutual Consistency of Hard Regions f...</td>\n",
       "      <td>[paper, proposed, novel, mutual, consistency, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Parameter Decoupling Strategy for Semi-supervi...</td>\n",
       "      <td>[consistency, training, proven, advanced, semi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Background-Foreground Segmentation for Interio...</td>\n",
       "      <td>[ensure, safety, automated, driving, correct, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              titles  \\\n",
       "0  Survey on Semantic Stereo Matching / Semantic ...   \n",
       "1  FUTURE-AI: Guiding Principles and Consensus Re...   \n",
       "2  Enforcing Mutual Consistency of Hard Regions f...   \n",
       "3  Parameter Decoupling Strategy for Semi-supervi...   \n",
       "4  Background-Foreground Segmentation for Interio...   \n",
       "\n",
       "                                           summaries  \n",
       "0  [stereo, matching, one, widely, used, techniqu...  \n",
       "1  [recent, advancements, artificial, intelligenc...  \n",
       "2  [paper, proposed, novel, mutual, consistency, ...  \n",
       "3  [consistency, training, proven, advanced, semi...  \n",
       "4  [ensure, safety, automated, driving, correct, ...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50b8b91f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(51774, 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "30dc5d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26774, 2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[5000:]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f3eb891",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=df[\"titles\"].values\n",
    "x=df[\"summaries\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fb070722",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train , y_test=train_test_split(x, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "012e26c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=Tokenizer(oov_token=\"nothing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1500b81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e567219e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'nothing': 12963,\n",
       " 'learning': 2,\n",
       " 'data': 3,\n",
       " 'model': 4,\n",
       " 'image': 5,\n",
       " 'network': 6,\n",
       " 'methods': 7,\n",
       " 'method': 8,\n",
       " 'propose': 9,\n",
       " 'models': 10,\n",
       " 'images': 11,\n",
       " 'proposed': 12,\n",
       " 'deep': 13,\n",
       " 'performance': 14,\n",
       " 'paper': 15,\n",
       " 'networks': 16,\n",
       " 'using': 17,\n",
       " 'results': 18,\n",
       " 'show': 19,\n",
       " 'neural': 20,\n",
       " 'training': 21,\n",
       " 'based': 22,\n",
       " 'approach': 23,\n",
       " 'graph': 24,\n",
       " 'detection': 25,\n",
       " 'features': 26,\n",
       " 'tasks': 27,\n",
       " 'problem': 28,\n",
       " 'information': 29,\n",
       " 'object': 30,\n",
       " 'datasets': 31,\n",
       " 'two': 32,\n",
       " 'different': 33,\n",
       " 'also': 34,\n",
       " 'new': 35,\n",
       " 'novel': 36,\n",
       " 'segmentation': 37,\n",
       " 'dataset': 38,\n",
       " 'attention': 39,\n",
       " 'time': 40,\n",
       " 'task': 41,\n",
       " 'however': 42,\n",
       " 'work': 43,\n",
       " 'used': 44,\n",
       " 'framework': 45,\n",
       " '3d': 46,\n",
       " 'stateoftheart': 47,\n",
       " 'experiments': 48,\n",
       " 'feature': 49,\n",
       " 'demonstrate': 50,\n",
       " 'algorithm': 51,\n",
       " 'use': 52,\n",
       " 'classification': 53,\n",
       " 'existing': 54,\n",
       " 'representation': 55,\n",
       " 'accuracy': 56,\n",
       " 'one': 57,\n",
       " 'first': 58,\n",
       " 'algorithms': 59,\n",
       " 'present': 60,\n",
       " 'learn': 61,\n",
       " 'adversarial': 62,\n",
       " 'approaches': 63,\n",
       " 'point': 64,\n",
       " '': 65,\n",
       " 'space': 66,\n",
       " 'large': 67,\n",
       " 'visual': 68,\n",
       " 'set': 69,\n",
       " 'input': 70,\n",
       " 'well': 71,\n",
       " 'video': 72,\n",
       " 'many': 73,\n",
       " 'recent': 74,\n",
       " 'convolutional': 75,\n",
       " 'prediction': 76,\n",
       " 'domain': 77,\n",
       " 'objects': 78,\n",
       " 'representations': 79,\n",
       " 'loss': 80,\n",
       " 'trained': 81,\n",
       " 'generative': 82,\n",
       " 'available': 83,\n",
       " 'better': 84,\n",
       " 'analysis': 85,\n",
       " 'applications': 86,\n",
       " 'reinforcement': 87,\n",
       " 'semantic': 88,\n",
       " 'architecture': 89,\n",
       " 'process': 90,\n",
       " 'multiple': 91,\n",
       " 'compared': 92,\n",
       " 'number': 93,\n",
       " 'introduce': 94,\n",
       " 'due': 95,\n",
       " 'recognition': 96,\n",
       " 'knowledge': 97,\n",
       " 'provide': 98,\n",
       " 'without': 99,\n",
       " 'problems': 100,\n",
       " 'function': 101,\n",
       " 'machine': 102,\n",
       " 'structure': 103,\n",
       " 'improve': 104,\n",
       " 'high': 105,\n",
       " 'study': 106,\n",
       " 'challenging': 107,\n",
       " 'local': 108,\n",
       " 'transfer': 109,\n",
       " 'depth': 110,\n",
       " 'human': 111,\n",
       " 'several': 112,\n",
       " 'various': 113,\n",
       " 'vision': 114,\n",
       " 'techniques': 115,\n",
       " 'series': 116,\n",
       " 'estimation': 117,\n",
       " 'given': 118,\n",
       " 'target': 119,\n",
       " 'design': 120,\n",
       " 'outperforms': 121,\n",
       " 'system': 122,\n",
       " 'achieve': 123,\n",
       " 'policy': 124,\n",
       " 'learned': 125,\n",
       " 'samples': 126,\n",
       " 'single': 127,\n",
       " 'address': 128,\n",
       " 'graphs': 129,\n",
       " 'three': 130,\n",
       " 'efficient': 131,\n",
       " 'module': 132,\n",
       " 'achieves': 133,\n",
       " 'systems': 134,\n",
       " 'experimental': 135,\n",
       " 'real': 136,\n",
       " 'research': 137,\n",
       " 'rl': 138,\n",
       " 'unsupervised': 139,\n",
       " 'code': 140,\n",
       " 'temporal': 141,\n",
       " 'scene': 142,\n",
       " 'important': 143,\n",
       " 'distribution': 144,\n",
       " 'extensive': 145,\n",
       " 'eg': 146,\n",
       " 'benchmark': 147,\n",
       " 'quality': 148,\n",
       " 'generate': 149,\n",
       " 'previous': 150,\n",
       " 'spatial': 151,\n",
       " 'often': 152,\n",
       " 'generation': 153,\n",
       " 'effective': 154,\n",
       " 'including': 155,\n",
       " 'natural': 156,\n",
       " 'complex': 157,\n",
       " 'recently': 158,\n",
       " 'optimization': 159,\n",
       " 'latent': 160,\n",
       " 'across': 161,\n",
       " 'state': 162,\n",
       " 'realworld': 163,\n",
       " 'parameters': 164,\n",
       " 'train': 165,\n",
       " 'simple': 166,\n",
       " 'effectiveness': 167,\n",
       " 'applied': 168,\n",
       " 'map': 169,\n",
       " 'significantly': 170,\n",
       " 'may': 171,\n",
       " 'evaluate': 172,\n",
       " 'via': 173,\n",
       " 'specifically': 174,\n",
       " 'global': 175,\n",
       " 'language': 176,\n",
       " 'generated': 177,\n",
       " 'inference': 178,\n",
       " 'class': 179,\n",
       " 'mechanism': 180,\n",
       " 'thus': 181,\n",
       " 'even': 182,\n",
       " 'order': 183,\n",
       " 'current': 184,\n",
       " 'robust': 185,\n",
       " 'supervised': 186,\n",
       " 'able': 187,\n",
       " 'face': 188,\n",
       " 'computer': 189,\n",
       " 'color': 190,\n",
       " 'cnn': 191,\n",
       " 'ie': 192,\n",
       " 'labels': 193,\n",
       " 'finally': 194,\n",
       " 'prior': 195,\n",
       " 'terms': 196,\n",
       " 'evaluation': 197,\n",
       " 'maps': 198,\n",
       " 'layers': 199,\n",
       " 'small': 200,\n",
       " 'perform': 201,\n",
       " 'significant': 202,\n",
       " 'architectures': 203,\n",
       " 'context': 204,\n",
       " 'gans': 205,\n",
       " 'source': 206,\n",
       " 'test': 207,\n",
       " 'accurate': 208,\n",
       " 'domains': 209,\n",
       " 'decision': 210,\n",
       " 'node': 211,\n",
       " 'regions': 212,\n",
       " 'processing': 213,\n",
       " 'standard': 214,\n",
       " 'synthetic': 215,\n",
       " 'modeling': 216,\n",
       " 'flow': 217,\n",
       " 'key': 218,\n",
       " 'pose': 219,\n",
       " 'action': 220,\n",
       " 'best': 221,\n",
       " 'transformer': 222,\n",
       " 'allows': 223,\n",
       " 'shown': 224,\n",
       " 'cloud': 225,\n",
       " 'motion': 226,\n",
       " 'gan': 227,\n",
       " 'field': 228,\n",
       " 'medical': 229,\n",
       " 'layer': 230,\n",
       " 'general': 231,\n",
       " 'focus': 232,\n",
       " 'find': 233,\n",
       " 'among': 234,\n",
       " 'limited': 235,\n",
       " 'challenge': 236,\n",
       " 'works': 237,\n",
       " 'achieved': 238,\n",
       " 'moreover': 239,\n",
       " 'particular': 240,\n",
       " 'way': 241,\n",
       " 'search': 242,\n",
       " '1': 243,\n",
       " 'control': 244,\n",
       " 'capture': 245,\n",
       " 'embedding': 246,\n",
       " 'computational': 247,\n",
       " 'ability': 248,\n",
       " 'provides': 249,\n",
       " 'still': 250,\n",
       " 'make': 251,\n",
       " 'within': 252,\n",
       " 'called': 253,\n",
       " 'clustering': 254,\n",
       " 'cost': 255,\n",
       " 'addition': 256,\n",
       " 'learns': 257,\n",
       " 'furthermore': 258,\n",
       " 'technique': 259,\n",
       " 'efficiency': 260,\n",
       " 'functions': 261,\n",
       " 'gradient': 262,\n",
       " '2': 263,\n",
       " 'optimal': 264,\n",
       " 'properties': 265,\n",
       " 'linear': 266,\n",
       " 'text': 267,\n",
       " 'shape': 268,\n",
       " 'points': 269,\n",
       " 'dynamic': 270,\n",
       " 'sample': 271,\n",
       " 'agents': 272,\n",
       " 'memory': 273,\n",
       " 'similar': 274,\n",
       " 'complexity': 275,\n",
       " 'agent': 276,\n",
       " 'sparse': 277,\n",
       " 'directly': 278,\n",
       " 'challenges': 279,\n",
       " 'generalization': 280,\n",
       " 'strategy': 281,\n",
       " 'endtoend': 282,\n",
       " 'develop': 283,\n",
       " 'understanding': 284,\n",
       " 'videos': 285,\n",
       " 'noise': 286,\n",
       " 'nodes': 287,\n",
       " 'uses': 288,\n",
       " 'solution': 289,\n",
       " 'result': 290,\n",
       " 'random': 291,\n",
       " 'environment': 292,\n",
       " 'setting': 293,\n",
       " 'classes': 294,\n",
       " 'error': 295,\n",
       " 'much': 296,\n",
       " 'ofthe': 297,\n",
       " 'robustness': 298,\n",
       " '2d': 299,\n",
       " 'size': 300,\n",
       " 'designed': 301,\n",
       " 'need': 302,\n",
       " 'fully': 303,\n",
       " 'obtain': 304,\n",
       " 'second': 305,\n",
       " 'benchmarks': 306,\n",
       " 'improves': 307,\n",
       " 'output': 308,\n",
       " 'since': 309,\n",
       " 'popular': 310,\n",
       " 'common': 311,\n",
       " 'range': 312,\n",
       " 'fusion': 313,\n",
       " 'level': 314,\n",
       " 'original': 315,\n",
       " 'clouds': 316,\n",
       " 'studies': 317,\n",
       " 'require': 318,\n",
       " 'success': 319,\n",
       " 'specific': 320,\n",
       " 'scale': 321,\n",
       " 'sequence': 322,\n",
       " 'frames': 323,\n",
       " 'sets': 324,\n",
       " 'future': 325,\n",
       " 'solve': 326,\n",
       " 'potential': 327,\n",
       " 'baseline': 328,\n",
       " 'years': 329,\n",
       " 'label': 330,\n",
       " 'like': 331,\n",
       " 'examples': 332,\n",
       " 'predictions': 333,\n",
       " 'step': 334,\n",
       " 'effectively': 335,\n",
       " 'low': 336,\n",
       " 'cnns': 337,\n",
       " 'predict': 338,\n",
       " 'requires': 339,\n",
       " 'wepropose': 340,\n",
       " 'camera': 341,\n",
       " 'application': 342,\n",
       " 'us': 343,\n",
       " 'case': 344,\n",
       " 'value': 345,\n",
       " 'labeled': 346,\n",
       " 'types': 347,\n",
       " 'obtained': 348,\n",
       " 'consider': 349,\n",
       " 'structures': 350,\n",
       " 'widely': 351,\n",
       " 'regression': 352,\n",
       " 'improvement': 353,\n",
       " 'instance': 354,\n",
       " 'objective': 355,\n",
       " 'improved': 356,\n",
       " 'traditional': 357,\n",
       " 'yet': 358,\n",
       " 'apply': 359,\n",
       " 'issue': 360,\n",
       " 'aims': 361,\n",
       " 'distance': 362,\n",
       " 'reduce': 363,\n",
       " 'especially': 364,\n",
       " 'convolution': 365,\n",
       " 'reward': 366,\n",
       " 'form': 367,\n",
       " 'highly': 368,\n",
       " 'main': 369,\n",
       " 'therefore': 370,\n",
       " 'sampling': 371,\n",
       " 'largescale': 372,\n",
       " 'enables': 373,\n",
       " 'environments': 374,\n",
       " 'reconstruction': 375,\n",
       " 'scenarios': 376,\n",
       " 'part': 377,\n",
       " 'possible': 378,\n",
       " 'scheme': 379,\n",
       " 'variety': 380,\n",
       " 'end': 381,\n",
       " 'manner': 382,\n",
       " 'known': 383,\n",
       " 'extract': 384,\n",
       " 'patterns': 385,\n",
       " 'optical': 386,\n",
       " 'promising': 387,\n",
       " 'matrix': 388,\n",
       " 'online': 389,\n",
       " 'instead': 390,\n",
       " 'good': 391,\n",
       " 'resolution': 392,\n",
       " 'art': 393,\n",
       " 'shows': 394,\n",
       " 'view': 395,\n",
       " 'settings': 396,\n",
       " 'could': 397,\n",
       " 'difficult': 398,\n",
       " 'pretrained': 399,\n",
       " 'matching': 400,\n",
       " 'interest': 401,\n",
       " 'similarity': 402,\n",
       " 'tracking': 403,\n",
       " 'explore': 404,\n",
       " 'higher': 405,\n",
       " 'generator': 406,\n",
       " 'scenes': 407,\n",
       " 'making': 408,\n",
       " 'classifier': 409,\n",
       " 'metric': 410,\n",
       " 'less': 411,\n",
       " 'theoretical': 412,\n",
       " 'theproposed': 413,\n",
       " 'weights': 414,\n",
       " 'adaptation': 415,\n",
       " 'efficiently': 416,\n",
       " 'related': 417,\n",
       " 'usually': 418,\n",
       " 'rate': 419,\n",
       " 'made': 420,\n",
       " 'lack': 421,\n",
       " 'gnns': 422,\n",
       " 'joint': 423,\n",
       " 'generating': 424,\n",
       " 'components': 425,\n",
       " 'superior': 426,\n",
       " 'competitive': 427,\n",
       " 'selfsupervised': 428,\n",
       " 'resulting': 429,\n",
       " 'dynamics': 430,\n",
       " 'leads': 431,\n",
       " 'despite': 432,\n",
       " 'conditions': 433,\n",
       " 'distributions': 434,\n",
       " 'selection': 435,\n",
       " 'diverse': 436,\n",
       " 'produce': 437,\n",
       " 'dense': 438,\n",
       " 'makes': 439,\n",
       " 'transformation': 440,\n",
       " 'useful': 441,\n",
       " 'metrics': 442,\n",
       " 'region': 443,\n",
       " 'evaluated': 444,\n",
       " 'recurrent': 445,\n",
       " 'embeddings': 446,\n",
       " 'wide': 447,\n",
       " 'augmentation': 448,\n",
       " 'power': 449,\n",
       " 'empirical': 450,\n",
       " 'corresponding': 451,\n",
       " 'great': 452,\n",
       " 'vector': 453,\n",
       " 'facial': 454,\n",
       " 'fast': 455,\n",
       " 'additional': 456,\n",
       " 'cases': 457,\n",
       " 'faster': 458,\n",
       " 'policies': 459,\n",
       " 'goal': 460,\n",
       " 'attacks': 461,\n",
       " 'multimodal': 462,\n",
       " 'hierarchical': 463,\n",
       " 'conditional': 464,\n",
       " 'constraints': 465,\n",
       " 'estimate': 466,\n",
       " 'computation': 467,\n",
       " 'either': 468,\n",
       " 'consists': 469,\n",
       " 'behavior': 470,\n",
       " 'solutions': 471,\n",
       " 'continuous': 472,\n",
       " 'kernel': 473,\n",
       " 'strong': 474,\n",
       " 'developed': 475,\n",
       " 'practical': 476,\n",
       " 'respectively': 477,\n",
       " 'speed': 478,\n",
       " 'convergence': 479,\n",
       " 'edge': 480,\n",
       " 'geometric': 481,\n",
       " 'improvements': 482,\n",
       " 'identify': 483,\n",
       " 'importance': 484,\n",
       " 'compare': 485,\n",
       " 'help': 486,\n",
       " 'adaptive': 487,\n",
       " 'investigate': 488,\n",
       " 'average': 489,\n",
       " 'forecasting': 490,\n",
       " 'user': 491,\n",
       " 'inputs': 492,\n",
       " 'world': 493,\n",
       " 'amount': 494,\n",
       " 'interactions': 495,\n",
       " 'frame': 496,\n",
       " 'attributes': 497,\n",
       " 'detect': 498,\n",
       " 'autonomous': 499,\n",
       " 'underlying': 500,\n",
       " 'thispaper': 501,\n",
       " 'jointly': 502,\n",
       " 'exploit': 503,\n",
       " 'timeseries': 504,\n",
       " 'parts': 505,\n",
       " 'towards': 506,\n",
       " 'inthis': 507,\n",
       " 'annotations': 508,\n",
       " 'driving': 509,\n",
       " 'contrast': 510,\n",
       " 'example': 511,\n",
       " 'issues': 512,\n",
       " 'actions': 513,\n",
       " 'changes': 514,\n",
       " 'tackle': 515,\n",
       " 'demonstrated': 516,\n",
       " 'presents': 517,\n",
       " 'rely': 518,\n",
       " 'uncertainty': 519,\n",
       " 'four': 520,\n",
       " 'simultaneously': 521,\n",
       " 'inthe': 522,\n",
       " 'variables': 523,\n",
       " 'translation': 524,\n",
       " 'baselines': 525,\n",
       " 'sequences': 526,\n",
       " 'values': 527,\n",
       " 'supervision': 528,\n",
       " 'critical': 529,\n",
       " 'content': 530,\n",
       " 'individual': 531,\n",
       " 'increasing': 532,\n",
       " 'leverage': 533,\n",
       " 'localization': 534,\n",
       " 'long': 535,\n",
       " 'performs': 536,\n",
       " 'proposes': 537,\n",
       " 'along': 538,\n",
       " 'person': 539,\n",
       " 'although': 540,\n",
       " 'introduced': 541,\n",
       " 'unseen': 542,\n",
       " 'automatically': 543,\n",
       " 'area': 544,\n",
       " 'hand': 545,\n",
       " 'support': 546,\n",
       " 'score': 547,\n",
       " 'realistic': 548,\n",
       " 'become': 549,\n",
       " 'analyze': 550,\n",
       " 'lower': 551,\n",
       " 'traffic': 552,\n",
       " 'reasoning': 553,\n",
       " 'powerful': 554,\n",
       " 'measure': 555,\n",
       " 'tree': 556,\n",
       " 'build': 557,\n",
       " 'regularization': 558,\n",
       " 'stochastic': 559,\n",
       " 'question': 560,\n",
       " 'compression': 561,\n",
       " 'gap': 562,\n",
       " 'detectors': 563,\n",
       " 'exploration': 564,\n",
       " 'bias': 565,\n",
       " 'discriminative': 566,\n",
       " 'detector': 567,\n",
       " 'inspired': 568,\n",
       " 'automatic': 569,\n",
       " 'background': 570,\n",
       " 'named': 571,\n",
       " 'crucial': 572,\n",
       " 'validate': 573,\n",
       " 'pixels': 574,\n",
       " 'variational': 575,\n",
       " 'extraction': 576,\n",
       " 'stage': 577,\n",
       " 'building': 578,\n",
       " 'relevant': 579,\n",
       " 'idea': 580,\n",
       " 'progress': 581,\n",
       " 'instances': 582,\n",
       " 'conventional': 583,\n",
       " 'typically': 584,\n",
       " 'imaging': 585,\n",
       " 'strategies': 586,\n",
       " 'take': 587,\n",
       " 'mapping': 588,\n",
       " 'appearance': 589,\n",
       " 'improving': 590,\n",
       " 'structured': 591,\n",
       " 'transform': 592,\n",
       " 'encoder': 593,\n",
       " 'component': 594,\n",
       " 'consistency': 595,\n",
       " 'observations': 596,\n",
       " '3': 597,\n",
       " 'signal': 598,\n",
       " 'advances': 599,\n",
       " 'increase': 600,\n",
       " 'style': 601,\n",
       " 'selfattention': 602,\n",
       " 'states': 603,\n",
       " 'aim': 604,\n",
       " 'lead': 605,\n",
       " 'saliency': 606,\n",
       " 'semisupervised': 607,\n",
       " 'full': 608,\n",
       " 'observed': 609,\n",
       " 'synthesis': 610,\n",
       " 'realtime': 611,\n",
       " 'signals': 612,\n",
       " 'public': 613,\n",
       " 'cannot': 614,\n",
       " 'nonlinear': 615,\n",
       " 'ground': 616,\n",
       " 'outperform': 617,\n",
       " 'transformations': 618,\n",
       " 'relationships': 619,\n",
       " 'effect': 620,\n",
       " 'role': 621,\n",
       " 'comparison': 622,\n",
       " 'fields': 623,\n",
       " 'extracted': 624,\n",
       " 'literature': 625,\n",
       " 'pairs': 626,\n",
       " 'development': 627,\n",
       " 'imagenet': 628,\n",
       " 'final': 629,\n",
       " 'unlabeled': 630,\n",
       " 'modules': 631,\n",
       " 'explicitly': 632,\n",
       " 'factors': 633,\n",
       " 'relations': 634,\n",
       " 'rgb': 635,\n",
       " 'comparable': 636,\n",
       " 'contrastive': 637,\n",
       " 'easily': 638,\n",
       " 'probability': 639,\n",
       " 'found': 640,\n",
       " 'empirically': 641,\n",
       " 'superresolution': 642,\n",
       " 'theory': 643,\n",
       " 'type': 644,\n",
       " 'explain': 645,\n",
       " 'then': 646,\n",
       " 'enhance': 647,\n",
       " 'handle': 648,\n",
       " 'users': 649,\n",
       " 'anomaly': 650,\n",
       " 'bounding': 651,\n",
       " 'publicly': 652,\n",
       " 'capable': 653,\n",
       " 'pixel': 654,\n",
       " 'conduct': 655,\n",
       " 'statistical': 656,\n",
       " 'retrieval': 657,\n",
       " 'performed': 658,\n",
       " 'utilize': 659,\n",
       " 'interaction': 660,\n",
       " 'attack': 661,\n",
       " 'mean': 662,\n",
       " 'solving': 663,\n",
       " 'downstream': 664,\n",
       " 'conducted': 665,\n",
       " 'major': 666,\n",
       " 'procedure': 667,\n",
       " 'essential': 668,\n",
       " 'noisy': 669,\n",
       " 'approximation': 670,\n",
       " 'pipeline': 671,\n",
       " 'discriminator': 672,\n",
       " 'applying': 673,\n",
       " 'advantage': 674,\n",
       " 'multiscale': 675,\n",
       " 'causal': 676,\n",
       " 'achieving': 677,\n",
       " 'impact': 678,\n",
       " 'gnn': 679,\n",
       " 'that': 680,\n",
       " 'predicting': 681,\n",
       " 'spaces': 682,\n",
       " 'views': 683,\n",
       " 'consistent': 684,\n",
       " 'gaussian': 685,\n",
       " 'structural': 686,\n",
       " 'processes': 687,\n",
       " 'remains': 688,\n",
       " 'ones': 689,\n",
       " 'fundamental': 690,\n",
       " 'texture': 691,\n",
       " 'represent': 692,\n",
       " 'bound': 693,\n",
       " 'alignment': 694,\n",
       " 'practice': 695,\n",
       " 'prove': 696,\n",
       " 'interpretable': 697,\n",
       " 'quantitative': 698,\n",
       " 'combination': 699,\n",
       " 'required': 700,\n",
       " 'categories': 701,\n",
       " 'learningbased': 702,\n",
       " 'areas': 703,\n",
       " 'bayesian': 704,\n",
       " 'modalities': 705,\n",
       " 'details': 706,\n",
       " 'provided': 707,\n",
       " 'parameter': 708,\n",
       " 'lidar': 709,\n",
       " 'respect': 710,\n",
       " 'edges': 711,\n",
       " 'brain': 712,\n",
       " 'levels': 713,\n",
       " 'discrete': 714,\n",
       " 'limitations': 715,\n",
       " 'scales': 716,\n",
       " 'combined': 717,\n",
       " 'trees': 718,\n",
       " 'certain': 719,\n",
       " 'expensive': 720,\n",
       " 'nature': 721,\n",
       " 'classifiers': 722,\n",
       " 'testing': 723,\n",
       " 'box': 724,\n",
       " 'ensemble': 725,\n",
       " 'hard': 726,\n",
       " 'geometry': 727,\n",
       " 'past': 728,\n",
       " 'open': 729,\n",
       " 'salient': 730,\n",
       " 'enable': 731,\n",
       " 'predictive': 732,\n",
       " 'dependencies': 733,\n",
       " 'namely': 734,\n",
       " 'query': 735,\n",
       " 'explanations': 736,\n",
       " 'additionally': 737,\n",
       " 'comprehensive': 738,\n",
       " 'group': 739,\n",
       " 'overall': 740,\n",
       " 'transformers': 741,\n",
       " 'advantages': 742,\n",
       " 'binary': 743,\n",
       " 'characteristics': 744,\n",
       " 'providing': 745,\n",
       " 'change': 746,\n",
       " 'suitable': 747,\n",
       " 'annotated': 748,\n",
       " 'block': 749,\n",
       " 'allow': 750,\n",
       " 'another': 751,\n",
       " 'encoding': 752,\n",
       " 'relationship': 753,\n",
       " 'generalize': 754,\n",
       " 'community': 755,\n",
       " 'autoencoder': 756,\n",
       " 'energy': 757,\n",
       " 'presented': 758,\n",
       " 'considered': 759,\n",
       " 'pretraining': 760,\n",
       " 'ii': 761,\n",
       " 'tothe': 762,\n",
       " 'operations': 763,\n",
       " 'decoder': 764,\n",
       " 'fail': 765,\n",
       " 'leading': 766,\n",
       " 'correlation': 767,\n",
       " 'dnns': 768,\n",
       " 'construct': 769,\n",
       " 'contextual': 770,\n",
       " 'extend': 771,\n",
       " 'etc': 772,\n",
       " 'steps': 773,\n",
       " 'commonly': 774,\n",
       " 'missing': 775,\n",
       " 'onthe': 776,\n",
       " 'spatiotemporal': 777,\n",
       " 'neuralnetworks': 778,\n",
       " 'overcome': 779,\n",
       " 'variations': 780,\n",
       " 'light': 781,\n",
       " 'takes': 782,\n",
       " 'property': 783,\n",
       " 'pooling': 784,\n",
       " 'perspective': 785,\n",
       " 'combining': 786,\n",
       " 'spectral': 787,\n",
       " 'suffer': 788,\n",
       " 'i': 789,\n",
       " 'markov': 790,\n",
       " 'leveraging': 791,\n",
       " 'sequential': 792,\n",
       " 'successfully': 793,\n",
       " 'unlike': 794,\n",
       " 'describe': 795,\n",
       " 'unified': 796,\n",
       " 'kitti': 797,\n",
       " 'fixed': 798,\n",
       " 'mechanisms': 799,\n",
       " 'clinical': 800,\n",
       " 'annotation': 801,\n",
       " 'poses': 802,\n",
       " 'fashion': 803,\n",
       " 'formulation': 804,\n",
       " 'discuss': 805,\n",
       " 'sensor': 806,\n",
       " 'hence': 807,\n",
       " 'probabilistic': 808,\n",
       " 'whether': 809,\n",
       " 'interpretability': 810,\n",
       " 'perception': 811,\n",
       " 'together': 812,\n",
       " 'concept': 813,\n",
       " 'times': 814,\n",
       " 'capability': 815,\n",
       " 'seen': 816,\n",
       " 'showthat': 817,\n",
       " 'according': 818,\n",
       " 'computing': 819,\n",
       " 'mask': 820,\n",
       " 'shapes': 821,\n",
       " 'associated': 822,\n",
       " 'generates': 823,\n",
       " 'accurately': 824,\n",
       " 'detecting': 825,\n",
       " 'density': 826,\n",
       " 'reduction': 827,\n",
       " 'upon': 828,\n",
       " 'mainly': 829,\n",
       " 'active': 830,\n",
       " 'rich': 831,\n",
       " 'automated': 832,\n",
       " 'understand': 833,\n",
       " 'cameras': 834,\n",
       " 'classical': 835,\n",
       " 'hidden': 836,\n",
       " 'diversity': 837,\n",
       " 'tool': 838,\n",
       " 'employ': 839,\n",
       " 'relative': 840,\n",
       " 'term': 841,\n",
       " 'concepts': 842,\n",
       " 'modern': 843,\n",
       " 'implementation': 844,\n",
       " 'combine': 845,\n",
       " 'artificial': 846,\n",
       " 'this': 847,\n",
       " 'truth': 848,\n",
       " 'raw': 849,\n",
       " 'compute': 850,\n",
       " 'monocular': 851,\n",
       " 'outputs': 852,\n",
       " 'humans': 853,\n",
       " 'alternative': 854,\n",
       " 'fewshot': 855,\n",
       " 'vectors': 856,\n",
       " 'rather': 857,\n",
       " 'derive': 858,\n",
       " 'sensors': 859,\n",
       " 'unknown': 860,\n",
       " 'risk': 861,\n",
       " 'highresolution': 862,\n",
       " 'arbitrary': 863,\n",
       " 'estimates': 864,\n",
       " 'captioning': 865,\n",
       " 'residual': 866,\n",
       " 'aggregation': 867,\n",
       " 'showing': 868,\n",
       " 'besides': 869,\n",
       " 'performing': 870,\n",
       " 'them': 871,\n",
       " 'considering': 872,\n",
       " 'deeplearning': 873,\n",
       " 'tools': 874,\n",
       " 'contains': 875,\n",
       " 'particularly': 876,\n",
       " 'superiority': 877,\n",
       " 'benefits': 878,\n",
       " 'shared': 879,\n",
       " 'collected': 880,\n",
       " 'paradigm': 881,\n",
       " 'independent': 882,\n",
       " 'whole': 883,\n",
       " 'numerical': 884,\n",
       " 'variable': 885,\n",
       " 'highquality': 886,\n",
       " 'devices': 887,\n",
       " 'finegrained': 888,\n",
       " 'far': 889,\n",
       " 'combines': 890,\n",
       " 'vehicle': 891,\n",
       " 'surface': 892,\n",
       " 'would': 893,\n",
       " 'thestateoftheart': 894,\n",
       " 'tested': 895,\n",
       " 'highdimensional': 896,\n",
       " 'margin': 897,\n",
       " 'weight': 898,\n",
       " 'patches': 899,\n",
       " 'effects': 900,\n",
       " 'highlevel': 901,\n",
       " 'initial': 902,\n",
       " 'studied': 903,\n",
       " 'motivated': 904,\n",
       " 'defined': 905,\n",
       " 'ai': 906,\n",
       " 'lstm': 907,\n",
       " 'registration': 908,\n",
       " 'approximate': 909,\n",
       " 'aspects': 910,\n",
       " 'allowing': 911,\n",
       " 'manifold': 912,\n",
       " 'variance': 913,\n",
       " 'deal': 914,\n",
       " 'game': 915,\n",
       " 'rewards': 916,\n",
       " 'measures': 917,\n",
       " 'following': 918,\n",
       " 'vehicles': 919,\n",
       " 'finding': 920,\n",
       " 'optimize': 921,\n",
       " 'negative': 922,\n",
       " 'al': 923,\n",
       " 'operation': 924,\n",
       " 'filter': 925,\n",
       " 'errors': 926,\n",
       " 'researchers': 927,\n",
       " 'games': 928,\n",
       " 'sensing': 929,\n",
       " 'channel': 930,\n",
       " 'estimating': 931,\n",
       " 'sr': 932,\n",
       " 'qualitative': 933,\n",
       " 'attribute': 934,\n",
       " 'expression': 935,\n",
       " 'boxes': 936,\n",
       " 'incorporate': 937,\n",
       " 'reduces': 938,\n",
       " 'introduces': 939,\n",
       " 'constraint': 940,\n",
       " 'generally': 941,\n",
       " 'phase': 942,\n",
       " 'flexible': 943,\n",
       " 'every': 944,\n",
       " 'faces': 945,\n",
       " 'precision': 946,\n",
       " 'captured': 947,\n",
       " 'successful': 948,\n",
       " 'pruning': 949,\n",
       " 'employed': 950,\n",
       " 'insights': 951,\n",
       " 'explicit': 952,\n",
       " 'distillation': 953,\n",
       " 'events': 954,\n",
       " 'scores': 955,\n",
       " 'social': 956,\n",
       " 'dnn': 957,\n",
       " 'relation': 958,\n",
       " 'adapt': 959,\n",
       " 'neuralnetwork': 960,\n",
       " 'multitask': 961,\n",
       " 'encode': 962,\n",
       " 'multiview': 963,\n",
       " 'bounds': 964,\n",
       " 'exploiting': 965,\n",
       " 'observation': 966,\n",
       " 'heterogeneous': 967,\n",
       " 'et': 968,\n",
       " 'guide': 969,\n",
       " 'per': 970,\n",
       " 'benefit': 971,\n",
       " 'larger': 972,\n",
       " 'factor': 973,\n",
       " 'create': 974,\n",
       " 'previously': 975,\n",
       " 'anovel': 976,\n",
       " 'next': 977,\n",
       " 'direction': 978,\n",
       " 'planning': 979,\n",
       " 'suggest': 980,\n",
       " 'disease': 981,\n",
       " 'location': 982,\n",
       " 'built': 983,\n",
       " 'contribution': 984,\n",
       " 'mobile': 985,\n",
       " 'account': 986,\n",
       " 'labeling': 987,\n",
       " 'observe': 988,\n",
       " 'finetuning': 989,\n",
       " 'embedded': 990,\n",
       " 'road': 991,\n",
       " 'focused': 992,\n",
       " 'privacy': 993,\n",
       " 'diagnosis': 994,\n",
       " 'physical': 995,\n",
       " 'normal': 996,\n",
       " 'distributed': 997,\n",
       " 'boundary': 998,\n",
       " 'auxiliary': 999,\n",
       " 'proposal': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad580e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "l=len(tokenizer.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a09887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=tokenizer.texts_to_sequences(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "96693bbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels=tokenizer.texts_to_sequences(y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "84a3fcdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len=max([len(i) for i in sequences])\n",
    "max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a7bce13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=pad_sequences(sequences, padding=\"post\", maxlen=max_len)\n",
    "labels=np.array(pad_sequences(labels, padding=\"post\", maxlen=25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f06c47be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "297189"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c145fba7",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 1.12 TiB for an array with shape (1035475, 297189) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m labels\u001b[38;5;241m=\u001b[39m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_categorical\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\np_utils.py:73\u001b[0m, in \u001b[0;36mto_categorical\u001b[1;34m(y, num_classes, dtype)\u001b[0m\n\u001b[0;32m     71\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(y) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     72\u001b[0m n \u001b[38;5;241m=\u001b[39m y\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 73\u001b[0m categorical \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m categorical[np\u001b[38;5;241m.\u001b[39marange(n), y] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m     75\u001b[0m output_shape \u001b[38;5;241m=\u001b[39m input_shape \u001b[38;5;241m+\u001b[39m (num_classes,)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 1.12 TiB for an array with shape (1035475, 297189) and data type float32"
     ]
    }
   ],
   "source": [
    "labels=keras.utils.to_categorical(labels, num_classes=l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7c69cc04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  303,    75,    13,    20,    16,  2436,  1856,   327, 25854,\n",
       "         208,     5,    37,    57,   369,   279, 29046,    16,     3,\n",
       "        1999,   876,  3960,  6292,    86,  2150,    37,    93, 74228,\n",
       "         152,   296,   551,    93, 34032,  3396,    21, 51894,     3,\n",
       "         605,   333,  3266,  2090,   506,  7107,   336,  1901,  1669,\n",
       "        6165,   364,  6084,    86,  1540,  4228,   296,   411, 40449,\n",
       "       13280,   112,     7,    12,   914, 51895,  2535,   371,    32,\n",
       "         334,    21,   271,  4193, 19546,    80,   261,    15,     9,\n",
       "        1069,  2973,    22, 19547,  1953,   128,   360,     3,  1999,\n",
       "        5766,   296,    84,  1199,   946,  1901,    21,    46,  3987,\n",
       "          13,    20,    16,   135,    18,    91, 74229,    37,  2726,\n",
       "        2754,    11,    19,   356, 25855,   547, 17073,   544,  9256,\n",
       "        2166,   207,     3,  1469,    18,   980, 19547,    80,   101,\n",
       "        1069,    45,  4391,   165,    13,    20,    16,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35af536a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41419, 287)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "74887683",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    Embedding(input_dim=(l-1),output_dim=10, input_length=max_len),\n",
    "    LSTM(units=128, return_sequences=True),\n",
    "    Dropout(0.1),\n",
    "    Dense(units=15, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9d16c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "482d50e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1332, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1316, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1297, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1074, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1028, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=categorical_crossentropy, and therefore expects target data to be provided in `fit()`.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_file8q01metu.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__train_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(step_function), (ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m), ag__\u001b[38;5;241m.\u001b[39mld(iterator)), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1332, in train_function  *\n        return step_function(self, iterator)\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1316, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1297, in run_step  **\n        outputs = model.train_step(data)\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1074, in train_step\n        self._validate_target_and_loss(y, loss)\n    File \"C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\engine\\training.py\", line 1028, in _validate_target_and_loss\n        raise ValueError(\n\n    ValueError: Target data is missing. Your model was compiled with loss=categorical_crossentropy, and therefore expects target data to be provided in `fit()`.\n"
     ]
    }
   ],
   "source": [
    "model.fit(sequences, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1512a23f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
